\section{Methods}
\label{ref_methods}

To understand and evaluate effective algorithms and techniques for outlier detection, a literature review is presented in this study. This review includes scientific literature, library documentation, and online resources. The goal of this review is to understand the cutting edge techniques and the logic behind the decision making of the algorithms. Additionally, a variety of interdisciplinary methods (ex. statistics, Deep Learning, etc.) are used to compare ideal use cases for building an anomaly detection pipeline and toolkit.  

A review is also presented that determines the most common datasets used to benchmark anomaly detection techniques. Since anomalies are by nature rare, most datasets contain a very small number of them. It is critical that the anomalies they do contain are a good representative sample. Results in Section \ref{ref_dataset_survey} show the most common datasets currently used in literature.   

Data collection is performed using the following procedure:
\begin{enumerate}
    \item Pre-process data inline with recommendations from the dataset authors.
    \item Setup the data processing pipelines using developed anomaly detector.
    \item Execute the pipeline for each experimental dataset
    \item Tune the detector parameters and optimize them for the specific dataset
    \item Collect and graph the results for anomaly detection analysis
\end{enumerate}

Data  analysis is performed by comparing the results of the detector against the experimental datasets. Through this analysis the benefits and shortcomings of each method can be understood. This analysis can also determine what type of detection methodology works best for the anomalies in this study. Using this analysis, the best method can be implemented for solving real-world interdisciplinary problems.

\subsection{Measuring Algorithms and Methods}

There will always be phenomena the algorithm cannot detect or behavior that is not anomalous that the algorithm detects accidentally. The goal is to optimize these behaviors for the given task. Therefore, many existing algorithms and implementations are being compared. This comparison will identify which algorithms perform the best for the task and will help identify which techniques are best for which types of data. This will provide insight on the existing shortcomings in the field and will show where the algorithms can be improved.

There are industry standard ways of comparing machine learning techniques. Some of these include ROC curves which allow you to compare false positive and true positive rates. There are also conventional statistical techniques which can be utilized. The researchers will explore further to determine which metrics are most appropriate for the task.

\subsection{Reliability, validity, and sensitivity aspects}
Anomalies will be identified (or pre-identified) in the dataset. Then the experimental results will be examined to see if the algorithm detected the anomalies correctly. When analyzing this data, the researchers will try to determine why certain false or true positives and negatives are occurring and propose techniques to mitigate them.

In the preliminary research considerations the researchers are analyzing and determining appropriate datasets for the experiments. One of the considerations is are the current industry standard datasets sufficient for benchmarking performance for modern algorithms. The researchers suspect that they are not, and thus will utilize a systematic technique to select appropriate and diverse datasets to adequately obtain valid algorithmic results that can be generalized to real world phenomena. 


\subsection{Resources}

The researchers have access to a variety of computational resources from universities and research institutions across Europe. In Finland the group has access to the CSC supercomputer. CSC provided some supercomputer resources and hosted cloud services available for the project. Additionally, the researchers have access to limited computational resources via their personal computers. Additionally, Each university in the consortium has access to additional compute resources that can be utilized by their respective members.

\subsection{Dataset Selection}
\label{ref_datasets}

In this research, three separate datasets will be used to test and evaluate the developed detection technique. The datasets span many fields of engineering and technology and provide a good survey of the generalizeability and applicability of the proposed detection technique.  

\subsubsection{Hydraulic Simulation Dataset}
\label{ref_hydraulic_dataset}

In this study, the Matlab software toolkit Simscape Multibody will be used to design a model of the hydraulic system outlined in figure \ref{fig:boom_structure}. This model will be used to study the behavior and response of the system to optimize behavior and system parameters. For this study, a mass m of 240 KG and a supply pressure of 185 bar is used.

\begin{figure}[H]
    %\centering
    \includegraphics[width=0.8\textwidth]{1_hydraulic_sim/BoomStructure.PNG}
    \caption{Hydraulic Boom Lift Structure}
    \label{fig:boom_structure}
\end{figure}


The control signal shown in figure \ref{fig:hydraulic_cs} is used as input to the simulation of the hydraulic cylinder. The control signal represents an approximate square wave profile. It stays at a neutral voltage of 0 until 0.2 seconds into the simulation. At 0.2 seconds, the control signal is at its maximum of 10 volts for 0.4 seconds. Then it falls to become proportionally negative at 0.7 seconds. After another 0.4 seconds elapsed, the signal returns to the neural position of 0 volts begenning at 1.1 seconds. 

\begin{figure}[H]
    %\centering
    \input{1_hydraulic_sim/results/input_sig_u.pgf}
    \caption{Hydraulic System Control Signal}
    \label{fig:hydraulic_cs}
\end{figure}

Figure \ref{fig:hydraulic_pos} shows the pressure for the system over time. Introducing a proportionally negative signal does not cause the end effector to return to its initial position. As the control signal is varied inversely, the position of the end effector only returns halfway to its initial position. This indicates the system exhibits a non-linear relationship between the control signal and the end effector position.    The system experiences mild oscillation after coming to rest when the control signal is back to zero at the end of the simulation. To achieve predictable system response and reduce oscillation, a more complex control methodology would be required.

\begin{figure}[H]
    %\centering
    \input{1_hydraulic_sim/results/input_sig_pos.pgf}
    \caption{Hydraulic Crane End Effector Position}
    \label{fig:hydraulic_pos}
\end{figure}

\subsubsection{Power Electronic Converter Dataset}
\label{ref_pec_dataset}

Transitioning from the conventional power systems to power electronics-dominated grids (PEDG) has increased demand for grid-forming converters (GFM) to facilitate operational reliability. Although significant research has been made on GFMs (as shown in Figure \ref{fig:sys}) to expedite stability under different grid conditions, its operation during faults or large signal disturbances still remain a challenge. Authors \cite{trainsient-stability-9523750} explain that GFMs handle a significantly smaller percentage of over-current (usually only 20\%) compared to synchronous generators (SGs) which can handle seven times their nominal current. This makes extreme fault detection and protection for GFMs critical while maintaining synchronization with the grid. Since the network infrastructure of power systems keeps expanding, it is important to identify these faults accurately under varying grid parameter uncertainties.

\begin{figure}[H]
	\includegraphics[width=0.65\textwidth]{Images/GFMSchema.pdf}
	\caption{Main circuit and control system structure of a grid-forming converter.}
	\label{fig:sys}
\end{figure}

In this dataset there are four faults considered. This study will examine the frequency [$f_c$] of the system throughout various fault conditions. Each fault has significantly different characteristics and magnitude. The faults are identified and explained in Table \ref{tab:pec_faults}.

\input{tables/pec_faults}

% \begin{figure}[H]
%     %\centering
%     \input{2_pec_sim/base_sig_f_c_all.pgf}
%     \caption{PEC Data Set $f_c$ Signal}
%     \label{fig:pec_all}
% \end{figure}

A line-to-line (LL) fault is also referred to as an unsymmetrical fault and occurs when there is a short circuit between two conductors. In three phase power, this can occur between two phases of the system. This fault causes a significant decrease in frequency that is orders of magnitude greater than the standard frequency of the system. 

During a three phase sensor fault, there is nothing wrong with the system itself but there is faulty sensor in the system. This causes the detected frequency to rise slightly. This slight rise is significantly less than the other examined faults and is very close to the reference frequency of 50 Hz. 

During a single phase voltage sag, the frequency oscillates continuously until the fault is over. This is a significant fault in the system and detection is critical to take remedial action. This is another fault where the magnitude is not very large in comparison to the LL Fault of the Three Phase Grid Fault.

A three phase grid fault is a severe fault where there is a problem with the grid and corrective action needs to be taken promptly. In this fault there is a large magnitude drop in frequency for the duration of the fault. This is similar behavior to the Three Phase Sensor Fault but the frequency change of the difference is orders of magnitude larger and in the negative direction.

\subsubsection{Cyber Security BETH Dataset}
\label{ref_beth_dataset}

On the Github Discussion Board \parencite{RiverGithub2022} for the popular River \parencite{2020river} Machine Learning library, I proposed using a streaming Local Outlier Factor (LOF) methodology for outlier detection. This method is not the most suitable for this type of outliers because it is not focused on contextual detection.  Through this discussion, the Authors of the BETH dataset \parencite{beth-dataset}, added it to a public repository at Imperial College London which provides easy access for researchers wishing to work on the dataset.

The BPF-extended tracking honeypot (BETH) cyber security dataset \parencite{beth-dataset} was released in 2021 and is still under active development and testing. A cybersecurity honeypot is a set of computer resources that present a benefit to a hacker if they are exploited but are actively being monitored by an organization or individual. This makes it an attractive dataset for this study since it reflects the current state of the art in cybersecurity and is one of the first to be designed for uncertainty analysis and anomaly detection. 

For this dataset, an ssh vulnerability is exploited where any password entered will allow a user to login. The system is running two auxillary containers to monitor traffic. The first is the Berkely Packet Filter (BPF) which examines OS process management calls. The second monitor logs DNS activity from the system. This data is collected and parsed over a series of trials to form the dataset.

Authors \cite{beth-dataset} present a number of advantages of this dataset that make it attractive for modern machine learning and anomaly detection research. This includes:
\begin{inlinelist}
    \item being a large and comprehensive cyber-security dataset
    \item containing modern attack vectors
    \item including benign and attack information for each host that is fully labeled.
\end{inlinelist}

Figure \ref{fig:beth_userid_all} shows the samples from the BETH dataset for the UserID parameter that is used in this study. This parameter is treated as a continuous data stream that corresponds with the measurement time. The spikes in the UserID parameter generally correspond with the labeled outliers.

\begin{figure}[H]
    %\centering
    \input{3_beth_sim/base_sig_userId_all.pgf}
    \caption{BETH Dataset Signal}
    \label{fig:beth_userid_all}
\end{figure}

In the BETH dataset, the honypots are deployed in a cloud environment because many major companies utilise compute through cloud providers. Therefore it is important to understand the attacks that are specifically scanning the cloud provider space compared to the prior datasets which are collected exclusively through on-prem servers. Typically one chooses a dataset:
\begin{inlinelist}
    \item to benchmark and highlight a technique in a specific area or
    \item demonstrate the capabilities and generalizability of a technique to other disciplines.
\end{inlinelist}
The BETH dataset is designed in a way that machine learning researchers looking for applications can find it useful. In this work it is used in this way, by approaching the anomaly classification problem from a different angle than traditional machine learning techniques.

\subsection{Algorithm Development}
    % \item Create a simulation dataset with pre-marked, simple anomalies (ex. sensor failure) 
    % \item Setup an algorithm pipeline to run the data through multiple streaming and non-streaming techniques and collect results
    % \item Select a real dataset and identify or use pre-identified markers for anomalies
    % \item Run the datasets through the algorithm pipeline and collect results
    % \item Add additional components (drift detection, etc) to the pipeline and record data results 

The code development and experimentation for this work is uploaded to a public Github Repository \parencite{BeattieGithub2022}. The Figures and results presented in section \ref{ref_results} are generated using this code. This section outlines how the final outlier detector was developed and some of the unsuccessful detection attempts along the way. Some of the datasets in the repository are private and as such cannot be included in the repository. The code to perform the analysis is included in the repository and can be adapted to fit similar datasets or problems.


\subsubsection{Unsuccessful Attempts}

Many libraries and algorithms outlined in section \ref{ref_code_libraries} were tested in this study. The first attempt utilized techniques from the river ML \parencite{2020river} and pysad \parencite{pysad} libraries.  The tested algorithms in the pysad library include IForestASD, LODA, RSHash, xStream, and Robust Random Cut Forest. The tested (and only) algorithm from the river library is Half Space Trees.

The algorithms demonstrated good performance for point-wise outliers on a simulated noisy sine wave. The algorithms was then tested against the contextual outliers in the Power Electronics Dataset and all techniques performed poorly. The outliers present in the dataets in this study are not pointwise but contextual shaplet outliers. This shows that the detection techniques for point-wise outliers are not sufficent for detecting contextual outliers.

New techniques were selected from libraries that met the following criteria:
\begin{inlinelist}
    \item does not require training of a model or previous knowledge of the data.
    \item operates in a streaming context
    \item can use a windowed or fix memory and processing allocation
\end{inlinelist}. 

This significantly reduces the list of available algorithms and libraries as many required model training and do not operate well outside a batch context. The author evaluated the banpei \parencite{banpei} library's Singular Spectrum Transformation (SST) and Hotelling methods and the library was unable to detect the desired anomalies. The author also evaluated the Gradient and Difference detectors in TSOD \parencite{tsod} and those algorithms were also unable to detect the contextual anomalies.

\subsubsection{Matrix Profile Detector}

The Matrix Profile algorithm for contextual outlier detection from the stumpy library \parencite{law2019stumpy} was selected for implementation. Initial testing with the multivariate matrix profile demonstrated good performance but was very computationally expensive and slow. Additionally, the library didn't facilitate an iterative computation for this method so each iteration would need to recompute the entire matrix profile across the whole window.

Subsequently, the univariate matrix profile is used in this study as it is faster and allows for iterative updates to the matrix which is significantly more efficient. Following successful initial experimentation, an outlier model class was created to compute the matrix profile for each iteration and predict whether the current event was an outlier. 

This technique is used in section \ref{ref_results} to detect outliers in a variety of situations. It requires adjusting three parameters based on the characteristics of the data being analyzed. This is much less than the traditional number of parameters required by most machine learning techniques. Additionally it relies completely on mathematical calculations and statics and is therefore explainable without reverse engineering. 

\begin{equation}
    \label{eqn:mp_decission}
    \epsilon_{[i]} = \max_{mp} \geq \left(\mu_{mp} + \left(\sigma_{mp} \cdot \alpha\right)\right)
\end{equation}


Let $ \alpha$ be the standard deviation ($\sigma$) multiplication factor in Equation \ref{eqn:mp_decission}. This equation is used to calculate anomalies by determining the current mean value and comparing if it is greater then or equal to a multiplier of the standard deviation of the matrix profile. This class is parameterized so the user can set the data window size, analysis window size, standard deviation multiplier, and other parameters to avoid false positive detection. Optional filters are included to not trigger the detector while it is still loading data into the analysis window and not trigger on recent faults to avoid redundant detection. 

\begin{equation}
    \label{eqn:mp_geq_zero}
    \lambda_{[i]} = \left| \epsilon_{[i]} - \max_{mp} \right| > 0.01
\end{equation}

Equation \ref{eqn:mp_geq_zero} shows an included filter to ensure that the currently calculated metric minus the current max value is greater than 0.01. If this condition is not true there is not enough information to conclude definitely that the point is an outlier. Therefore it should not be selected as an outlier.

\subsubsection{Rolling Range Filter}

In certain datasets, like the BETH dataset, performance was inadequate without an additional filter. A significant number of false positives were detected without the implementation of this rolling range filter.

\begin{equation}
    \label{eqn:mp_rolling_range}
    \gamma_{[i]} = \max_{mp} - \min_{mp} > \left(\mu_{\epsilon[(n-5)...n]} \cdot \beta \right)
\end{equation}

Let $ \beta$ be the rolling range average ($\mu$) multiplication factor in Equation \ref{eqn:mp_rolling_range}. This equation describes the developed rolling range detector. In every iteration the range in the window is computed by subtracting the maximum and minimum value. A ring buffer is implemented to store the rolling range values. In this buffer, 5 elements are allowed in the buffer and when a new element is inserted the oldest element is purged. This means that only the 5 most recent elements that were detected will be available at once.

If the current range is greater than the average of the rolling range ring buffer multiplied by the standard deviation multiplier, then it is considered an anomaly. If this is the case the current range is appended to the rolling range ring buffer. This improves performance significantly for datasets like the hydraulic simulation and BETH dataset.

\begin{equation}
    \label{eqn:mp_cumulative}
        \text{anomaly}_{[i]}= 
    \begin{cases}
        true,& \text{if } \epsilon_{[i]} \land \lambda_{[i]} \land \gamma_{[i]}\\
        false,              & \text{otherwise}
    \end{cases}
\end{equation}