\section{Methods}
\label{ref_methods}

To understand and evaluate effective algorithms and techniques for outlier detection, a literature review is presented in this study. This review includes scientific literature, library documentation, and online resources. The goal of this review is to understand the cutting edge techniques and the logic behind the decision-making of the algorithms. Additionally, a variety of interdisciplinary methods (ex. statistics, Deep Learning, etc.) are used to compare ideal use cases for building an anomaly detection pipeline and toolkit.

A review is also presented that determines the most common datasets used to benchmark anomaly detection techniques. Since anomalies are by nature rare, most datasets contain a very small number of them. It is critical that the anomalies they do contain are a good representative sample. Results in Section \ref{ref_dataset_survey} show the most common datasets currently used in literature.

Data collection is performed using the following procedure:
\begin{enumerate}
    \item Pre-process data inline with recommendations from the dataset authors.
    \item Setup the data processing pipelines using the developed anomaly detector.
    \item Execute the pipeline for each experimental dataset
    \item Tune the detector parameters and optimize for the specific dataset
    \item Collect and graph the results for anomaly detection analysis
\end{enumerate}

Data analysis is performed by comparing the results of the detector against the ground truth for the experimental datasets. Through this analysis the benefits and shortcomings of each method can be understood. This analysis can also determine what type of detection methodology works best for the anomalies in this study. Using this analysis, the best method can be implemented for solving real-world interdisciplinary problems.

\subsection{Measuring Algorithms and Methods}

Algorithms are susceptible to missed detections from challenging to detect phenomena or false positives from non-anomalous phenomena. The goal is to minimize these false detections while prioritizing missed detections. If a detection is missed that is much more significant than a false positive because a false positive can be ignored but a missed detection can not be identified. Because of this, many existing algorithms and implementations are being compared to identify the best performing algorithms for the contextual outliers present in the studied datasets. This provides insight on the existing shortcomings in the field and shows where the algorithms can be improved.

There are industry standard ways of comparing machine learning techniques. Some of these include ROC metrics which allow you to compare false positive and true positive rates. There are also conventional statistical techniques which can be utilized. The most significant metrics for this study are true positive detection rate. For a detector to be considered successful, a 100\% true-positive identification rate is requried. False positive detection rate is also important to ensure the detector is not too noisy.

\subsection{Resources}

The researchers have access to a variety of computational resources from universities and research institutions across Europe. In Finland the group has access to the CSC supercomputer. CSC provided supercomputer resources and hosted cloud services available for the project. Additionally, the researchers have access to limited computational resources via their personal computers. Additionally, each university in the consortium has access to additional compute resources that can be utilized by their respective members.

\subsection{Dataset Survey}
\label{ref_dataset_survey}

In this section, the researchers surveyed 8 works focusing on streaming data outlier detection in machine learning. The datasets used in each paper were examined to determine which datasets were commonly used in the literature as shown in Table \ref{tab:datasets_outlier_detection}.

\input{tables/datasets}

Figure \ref{fig_dataset_lit} shows the most commonly occurring datasets in the literature from table \ref{tab:datasets_outlier_detection} are KDD-CUP99 \parencite{kdd1999} followed by Covertype-Forest \parencite{covertype-dataset}. A dataset is included in table \ref{tab:datasets_outlier_detection} and figure \ref{fig_dataset_lit} if there are 2 or more occurrences of it in the literature surveyed.

\begin{figure}[H]
    %%\centering
    \input{Images/dataset_survey.pgf}
    \caption{Dataset Occurrence in Literature }
    \label{fig_dataset_lit}
\end{figure}

The most commonly used datasets found in the literature were originally created in the late 1990's. Most of the research surveyed has been conducted in the last 10 years but the standard benchmark datasets are over two decades old. This data was collected when the landscape of cyber-attacks and computing looked much different than today and is therefore not sufficient to benchmark performance against modern, sophisticated attacks. This suggests a lack of standardization in datasets to compare algorithm performance.

After analysis of the existing datasets, we have determined appropriate datasets for the experiments. The current industry standard datasets are not sufficient for benchmarking performance in relation to the use cases presented in this study. In the subsequent section, we select appropriate and diverse datasets to benchmark specific outlier phenomenon that can be generalized to real world phenomena.

\subsection{Dataset Selection}
\label{ref_datasets}

In this research, three separate datasets are used to test and evaluate the developed detection technique. The datasets span many fields of engineering and technology and provide a good survey of the generalizeability and applicability of the proposed detection technique.

\subsubsection{Hydraulic Simulation Dataset}
\label{ref_hydraulic_dataset}

In this study, the Matlab software toolkit Simscape Multibody is used to design a model of the hydraulic system outlined in Figure \ref{fig:boom_structure}. This model is used to study the behavior and response of the system to optimize behavior and system parameters. For this study, a mass [$m$] of 240 kg and a supply pressure of 185 bar is used.

\begin{figure}[H]
    %\centering
    \includegraphics[width=\textwidth]{1_hydraulic_sim/BoomStructure.PNG}
    \caption{Hydraulic Boom Lift Structure}
    \label{fig:boom_structure}
\end{figure}


The control signal shown in Figure \ref{fig:hydraulic_cs} is used as input to the simulation of the hydraulic cylinder. The control signal represents an approximate square wave profile. It stays at a neutral voltage of 0 until 0.2 seconds into the simulation. At 0.2 seconds, the control signal is at its maximum of 10 volts for 0.4 seconds. Then it falls to become proportionally negative at 0.7 seconds. After another 0.4 seconds elapsed, the signal returns to the neural position of 0 volts begenning at 1.1 seconds.

\begin{figure}[H]
        %\centering
    \input{1_hydraulic_sim/results/input_sig_u.pgf}
    \caption{Hydraulic System Control Signal}
    \label{fig:hydraulic_cs}
\end{figure}

Figure \ref{fig:hydraulic_pos} shows the pressure for the system over time. Introducing a proportionally negative signal does not cause the end effector to return to its initial position. As the control signal is varied inversely, the position of the end effector only returns halfway to its initial position. This indicates the system exhibits a non-linear relationship between the control signal and the end effector position. The system experiences mild oscillation after coming to rest when the control signal is back to zero at the end of the simulation. To achieve predictable system response and reduce oscillation, a more complex control methodology would be required.

\begin{figure}[H]
        %\centering
    \input{1_hydraulic_sim/results/input_sig_pos.pgf}
    \caption{Hydraulic Crane End Effector Position}
    \label{fig:hydraulic_pos}
\end{figure}

\subsubsection{Power Electronic Converter Dataset}
\label{ref_pec_dataset}

Transitioning from the conventional power systems to power electronics-dominated grids (PEDG) has increased demand for grid-forming converters (GFM) to facilitate operational reliability. GFMs have made significant progress in recent years % (as shown in Figure \ref{fig:sys})
to expedite stability under different grid conditions, but their operation during faults or large signal disturbances still remain a challenge.

Authors \cite{trainsient-stability-9523750} explain that GFMs handle a significantly smaller percentage of over-current (usually only 20\%) compared to synchronous generators (SGs) which can handle seven times their nominal current. This makes extreme fault detection and protection for GFMs critical while maintaining synchronization with the grid. Since the network infrastructure of power systems keeps expanding, it is important to identify these faults accurately under varying grid parameter uncertainties.

% \begin{figure}[H]
% 	\includegraphics[width=\textwidth]{Images/GFMSchema.pdf}
% 	\caption{Main circuit and control system structure of a grid-forming converter.}
% 	\label{fig:sys}
% \end{figure}

Four faults are considered in the PEC dataset. This study examines the frequency [$f_c$] of the system throughout various fault conditions. Each fault has significantly different characteristics and magnitude. The faults exaimned in the study are explained in Table \ref{tab:pec_faults_table}.

\input{tables/pec_faults}

The faults explained in Table \ref{tab:pec_faults_table} are presented visually in Figure \ref{fig:pec-faults-overall-fig}.
 
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\input{"2_pec_sim/base_sig_f_c_f1.pgf"}}
        \caption{Line-to-Line (LL) Fault}  
        \label{fig:ll-fault}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \resizebox{\textwidth}{!}{\input{"2_pec_sim/base_sig_f_c_f2.pgf"}}
        \caption{Three-Phase Sensor Fault}  
        \label{fig:three-phase-sensor-fault}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \resizebox{\textwidth}{!}{\input{"2_pec_sim/base_sig_f_c_f3.pgf"}}
        \caption{Single-Phase Voltage Sag}    
        \label{fig:single-phase-voltage-sag}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \resizebox{\textwidth}{!}{\input{"2_pec_sim/base_sig_f_c_f4.pgf"}}
        \caption{Three-Phase Grid Fault}     
        \label{fig:three-phase-grid-fault}
    \end{subfigure}
    \caption{PEC Dataset Fault Visualization}  
    \label{fig:pec-faults-overall-fig}
\end{figure}


\subsubsection{Cyber Security BETH Dataset}
\label{ref_beth_dataset}

On the Github Discussion Board \parencite{RiverGithub2022} for the popular River \parencite{2020river} Machine Learning library, I proposed using a streaming Local Outlier Factor (LOF) methodology for outlier detection. Through this discussion, Authors \cite{beth-dataset}, added the BETH dataset to a public repository at Imperial College London which provides easy access for researchers wishing to work on the dataset. The LOF method proved to be unsuitable for the contextual outliers in this study and is not implemented in the River library. 

The BPF-extended tracking honeypot (BETH) cyber security dataset \parencite{beth-dataset} was released in 2021 and is still under active development and testing. A cybersecurity honeypot is a set of computer resources that present a benefit to a hacker if they are exploited but are actively being monitored by an organization or individual. This makes it an attractive dataset for this study since it reflects the current state of the art in cybersecurity and is one of the first to be designed for uncertainty analysis and anomaly detection.

For this dataset, an ssh vulnerability is exploited where any password entered allows a user to login. The system is running two auxillary containers to monitor traffic. The first is the Berkely Packet Filter (BPF) which examines OS process management calls. The second monitor logs DNS activity from the system. This data is collected and parsed over a series of trials to form the dataset.

Authors \cite{beth-dataset} present a number of advantages of this dataset that make it attractive for modern machine learning and anomaly detection research. This includes:
\begin{inlinelist}
    \item being a large and comprehensive cyber-security dataset
    \item containing modern attack vectors
    \item including benign and attack information for each host that is fully labeled.
\end{inlinelist}

Figure \ref{fig:beth_userid_all} shows the samples from the BETH dataset for the UserID parameter that is used in this study. This parameter is treated as a continuous data stream that corresponds with the measurement time. The spikes in the UserID parameter generally correspond with the labeled outliers.

\begin{figure}[H]
        %\centering
    \input{3_beth_sim/base_sig_userId_all.pgf}
    \caption{BETH Dataset Signal}
    \label{fig:beth_userid_all}
\end{figure}

In the BETH dataset, the honypots are deployed in a cloud environment because many major companies utilise compute through cloud providers. Therefore it is important to understand the attacks that are specifically scanning the cloud provider space compared to the prior datasets which are collected exclusively through on-prem servers. Typically one chooses a dataset:
\begin{inlinelist}
    \item to benchmark and highlight a technique in a specific area or
    \item demonstrate the capabilities and generalizability of a technique to other disciplines.
\end{inlinelist}
The BETH dataset is designed to be useful and easy to work with to machine learning researchers by providing large volumes of fully labelled data with an easy to parse format and understandable data points. In this work, the anomaly classification problem is approached from a different angle than traditional machine learning techniques which shows the generalizability of the dataset.

\subsection{Algorithm Development}
% \item Create a simulation dataset with pre-marked, simple anomalies (ex. sensor failure)
% \item Setup an algorithm pipeline to run the data through multiple streaming and non-streaming techniques and collect results
% \item Select a real dataset and identify or use pre-identified markers for anomalies
% \item Run the datasets through the algorithm pipeline and collect results
% \item Add additional components (drift detection, etc) to the pipeline and record data results

The code development and experimentation for this work is uploaded to a public GitHub repository \parencite{BeattieGithub2022}. The Figures and results presented in section \ref{ref_results} are generated using this code. This section outlines how the final outlier detector was developed and some of the unsuccessful detection attempts along the way. Some of the datasets in the repository are private and as such cannot be included in the repository. The code to perform the analysis is included in the repository and can be adapted to fit similar datasets or problems.

\subsubsection{Unsuccessful Attempts}

Many libraries and algorithms outlined in Section \ref{ref_code_libraries} were tested in this study. The first attempt utilized techniques from the river ML \parencite{2020river} and pysad \parencite{pysad} libraries. The tested algorithms in the pysad library include IForestASD, LODA, RSHash, xStream, and Robust Random Cut Forest. The tested (and only) algorithm from the river library is Half Space Trees.

The algorithms demonstrated good performance for point-wise outliers on a simulated noisy sine wave. The algorithms was then tested against the contextual outliers in the Power Electronics Dataset and all techniques performed poorly. The outliers present in the dataets in this study are not pointwise but contextual shaplet outliers.
% I think this is an important poit, and you may tell the reader what is the difference of these "anomalies" in a nutshell.
% At this point, the author realized that the kind of anomalies
% present in the Power Electronics Dataset (and other datasets in this study) are not
% pointwise but are instead contextual shaplet outliers. A new detection methodology
% would be
This shows that the detection techniques for point-wise outliers are not sufficent for detecting contextual outliers.

New techniques were selected from libraries that met the following criteria:
\begin{inlinelist}
    \item does not require training of a model or previous knowledge of the data.
    \item operates in a streaming context
    \item can use a windowed or fix memory and processing allocation
\end{inlinelist}.

This significantly reduces the list of available algorithms and libraries as many required model training and do not operate well outside a batch context. The author evaluated the banpei \parencite{banpei} library's Singular Spectrum Transformation (SST) and Hotelling methods and the library was unable to detect the desired anomalies. The author also evaluated the Gradient and Difference detectors in TSOD \parencite{tsod} which were also unable to detect the contextual anomalies.

\subsubsection{Matrix Profile Detector}

The Matrix Profile algorithm for contextual outlier detection from the STUMPY library \parencite{law2019stumpy} was selected for implementation. Initial testing with the multivariate matrix profile demonstrated good performance but was very computationally expensive and slow. Additionally, the library did not facilitate an iterative computation for this method so each iteration would need to recompute the entire matrix profile across the whole window.

Subsequently, the univariate matrix profile is used in this study as it is faster and allows for iterative updates to the matrix which is significantly more efficient. Following successful initial experimentation, an outlier model class was created to compute the matrix profile for each iteration and predict whether the current event was an outlier.

This technique is used in Section \ref{ref_results} to detect outliers in a variety of situations. It requires adjusting three parameters based on the characteristics of the data being analyzed. This is less than the conventional number of parameters required by machine learning techniques. Additionally it relies strictly on statics and is therefore explainable without reverse engineering.

\begin{equation}
    \label{eqn:mp_decission}
    \epsilon_{[i]} = \max_{mp} \geq \left(\mu_{mp} + \left(\sigma_{mp} \cdot \alpha\right)\right)
\end{equation}


Let $\alpha$ be the standard deviation ($\sigma$) multiplication factor in Equation \eqref{eqn:mp_decission}. This equation is used to calculate anomalies by determining the current mean value and comparing it against a multiplier of the standard deviation of the matrix profile. This equation is parameterized in code so the user can set the data window size, analysis window size, standard deviation multiplier, and other parameters to avoid false positive detection. Optional filters are included to avoid triggering while the detector is still loading data into the analysis window and on recent faults to avoid redundant detection.

\subsubsection{Detection Filters}

\begin{equation}
    \label{eqn:mp_geq_zero}
    \lambda_{[i]} = \left| \epsilon_{[i]} - \max_{mp} \right| > 0.01
\end{equation}

Equation \eqref{eqn:mp_geq_zero} shows a filter for the detector to ensure that the currently calculated metric ($\epsilon$) is representative enough to trigger a detection. If this condition is not true there is not enough information to conclude definitely that the point is an outlier. Therefore it should not be selected as an outlier.

In certain datasets, like the BETH dataset, performance was inadequate without filtering. A significant number of false positives were detected before the implementation of the rolling range filter.

\begin{equation}
    \label{eqn:mp_rolling_range}
    \gamma_{[i]} = \max_{mp} - \min_{mp} > \left(\mu_{\epsilon[(n-5)...n]} \cdot \beta \right)
\end{equation}

In Equation \eqref{eqn:mp_rolling_range}, the rolling range average ($\mu$) is multiplied by a scaling factor ($\beta$). The range in the window is computed during every iteration by subtracting the maximum and minimum value. A ring buffer is implemented to store the rolling range values. In this buffer, 5 elements are allowed in the buffer and when a new element is inserted the last element is purged. This means that only the 5 most recent elements that were detected are available at a time.

If the current range is greater than the average of the rolling range ring buffer multiplied by the standard deviation multiplier, then it is considered an anomaly. If this is the case the current range is appended to the rolling range ring buffer.

\subsubsection{Triggering Detection}

Using the matrix profile and the filters developed above, each data point is evaluated and determined to be anomalous or normal.

\begin{equation}
    \label{eqn:mp_cumulative}
    \text{anomaly}_{[i]}=
    \begin{cases}
        true,& \text{if } \epsilon_{[i]} \land \lambda_{[i]} \land \gamma_{[i]}\\
        false,              & \text{otherwise}
    \end{cases}
\end{equation}

The values are calculated using Equations \eqref{eqn:mp_decission}, \eqref{eqn:mp_geq_zero}, and \eqref{eqn:mp_rolling_range} respectively. To be determined anomalous, the point ($i$) must satisfy all the conditions in Equation \eqref{eqn:mp_cumulative}. If a point does not satisfy these conditions it is not considered an anomaly and it does not trigger the detector. The combination of the matrix profile algorithm and targeted filters allows the developed detector to be flexible and extensible to a wide range of datasets and window sizes as shown in Section \ref{ref_results}.
