@INPROCEEDINGS{three-layer-approach,
  author={Gutierrez-Rojas, Daniel and Ullah, Mehar and Christou, Ioannis T. and Almeida, Gustavo and Nardelli, Pedro and Carrillo, Dick and Sant’Ana, Jean M. and Alves, Hirley and Dzaferagic, Merim and Chiumento, Alessandro and Kalalas, Charalampos},
  booktitle={2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)}, 
  title={Three-layer Approach to Detect Anomalies in Industrial Environments based on Machine Learning}, 
  year={2020},
  volume={1},
  number={},
  pages={250-256},
  doi={10.1109/ICPS48405.2020.9274780}}
@ARTICLE{massive-machine,  author={Dawy, Zaher and Saad, Walid and Ghosh, Arunabha and Andrews, Jeffrey G. and Yaacoub, Elias},  journal={IEEE Wireless Communications},   title={Toward Massive Machine Type Cellular Communications},   year={2017},  volume={24},  number={1},  pages={120-128},  doi={10.1109/MWC.2016.1500284WC}}
@ARTICLE{compression,  author={Li, Shancang and Xu, Li Da and Wang, Xinheng},  journal={IEEE Transactions on Industrial Informatics},   title={Compressed Sensing Signal and Data Acquisition in Wireless Sensor Networks and Internet of Things},   year={2013},  volume={9},  number={4},  pages={2177-2186},  doi={10.1109/TII.2012.2189222}}
@article{tennessee-eastman-model,
title = {Implementations of the Tennessee Eastman Process in Modelica},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {2},
pages = {619-624},
year = {2018},
note = {9th Vienna International Conference on Mathematical Modelling},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.03.105},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318301095},
author = {Carla Martin-Villalba and Alfonso Urquia and Guodong Shao},
keywords = {Computer simulation, Dynamic modeling, Process models, Process simulators, Modelica},
abstract = {The Tennessee Eastman process is a typical industrial process that consists of five main process units: a two-phase reactor where an exothermic reaction occurs, a separator, a stripper, a compressor, and a mixer. This is a nonlinear open-loop unstable process that has been used in many studies as a case study for plant-wide control, statistical process monitoring, sensor fault detection, and identification of data-driven network models. There are many implementations of this process with different levels of detail, but there is no implementation developed in an object-oriented modeling language such as Modelica. This paper presents the development of two different Modelica libraries, TEprocess and TESimplified, that facilitate the implementation of the Tennesse Eastman process. These two libraries have been validated using data available in the literature.}
}
@INPROCEEDINGS{black-box-explainability,  author={Sahoo, Subham and Wang, Huai and Blaabjerg, Frede},  booktitle={2021 IEEE Energy Conversion Congress and Exposition (ECCE)},   title={On the Explainability of Black Box Data-Driven Controllers for Power Electronic Converters},   year={2021},  volume={},  number={},  pages={1366-1372},  doi={10.1109/ECCE47101.2021.9595231}}
@misc{explaining-adversarial-examples,
  doi = {10.48550/ARXIV.1412.6572},
  url = {https://arxiv.org/abs/1412.6572},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Explaining and Harnessing Adversarial Examples},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@Article{anomalies-detection-isolation,
AUTHOR = {Togbe, Maurras Ulbricht and Chabchoub, Yousra and Boly, Aliou and Barry, Mariam and Chiky, Raja and Bahri, Maroua},
TITLE = {Anomalies Detection Using Isolation in Concept-Drifting Data Streams },
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {13},
URL = {https://www.mdpi.com/2073-431X/10/1/13},
ISSN = {2073-431X},
ABSTRACT = {Detecting anomalies in streaming data is an important issue for many application domains, such as cybersecurity, natural disasters, or bank frauds. Different approaches have been designed in order to detect anomalies: statistics-based, isolation-based, clustering-based, etc. In this paper, we present a structured survey of the existing anomaly detection methods for data streams with a deep view on Isolation Forest (iForest). We first provide an implementation of Isolation Forest Anomalies detection in Stream Data (IForestASD), a variant of iForest for data streams. This implementation is built on top of scikit-multiflow (River), which is an open source machine learning framework for data streams containing a single anomaly detection algorithm in data streams, called Streaming half-space trees. We performed experiments on different real and well known data sets in order to compare the performance of our implementation of IForestASD and half-space trees. Moreover, we extended the IForestASD algorithm to handle drifting data by proposing three algorithms that involve two main well known drift detection methods: ADWIN and KSWIN. ADWIN is an adaptive sliding window algorithm for detecting change in a data stream. KSWIN is a more recent method and it refers to the Kolmogorov\&ndash;Smirnov Windowing method for concept drift detection. More precisely, we extended KSWIN to be able to deal with n-dimensional data streams. We validated and compared all of the proposed methods on both real and synthetic data sets. In particular, we evaluated the F1-score, the execution time, and the memory consumption. The experiments show that our extensions have lower resource consumption than the original version of IForestASD with a similar or better detection efficiency.},
DOI = {10.3390/computers10010013}
}

@inproceedings{dilof-data-streams,
author = {Na, Gyoung S. and Kim, Donghyun and Yu, Hwanjo},
title = {DILOF: Effective and Memory Efficient Local Outlier Detection in Data Streams},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220022},
doi = {10.1145/3219819.3220022},
abstract = {With precipitously growing demand to detect outliers in data streams, many studies have been conducted aiming to develop extensions of well-known outlier detection algorithm called Local Outlier Factor (LOF), for data streams. However, existing LOF-based algorithms for data streams still suffer from two inherent limitations: 1) Large amount of memory space is required. 2) A long sequence of outliers is not detected. In this paper, we propose a new outlier detection algorithm for data streams, called DILOF that effectively overcomes the limitations. To this end, we first develop a novel density-based sampling algorithm to summarize past data and then propose a new strategy for detecting a sequence of outliers. It is worth noting that our proposing algorithms do not require any prior knowledge or assumptions on data distribution. Moreover, we accelerate the execution time of DILOF about 15 times by developing a powerful distance approximation technique. Our comprehensive experiments on real-world datasets demonstrate that DILOF significantly outperforms the state-of-the-art competitors in terms of accuracy and execution time. The source code for the proposed algorithm is available at our website: http://di.postech.ac.kr/DILOF.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1993–2002},
numpages = {10},
keywords = {data streams, density-based sampling, outlier detection},
location = {London, United Kingdom},
series = {KDD '18}
}
@INPROCEEDINGS{fast-memory-efficent-lof-milof,  author={Salehi, Mahsa and Leckie, Christopher and Bezdek, James C. and Vaithianathan, Tharshan and Zhang, Xuyun},  booktitle={2017 IEEE 33rd International Conference on Data Engineering (ICDE)},   title={Fast Memory Efficient Local Outlier Detection in Data Streams (Extended Abstract)},   year={2017},  volume={},  number={},  pages={51-52},  doi={10.1109/ICDE.2017.32}}
@inproceedings{fast-anomaly-detection-streaming,
author = {Tan, Swee Chuan and Ting, Kai Ming and Liu, Tony Fei},
title = {Fast Anomaly Detection for Streaming Data},
year = {2011},
isbn = {9781577355144},
publisher = {AAAI Press},
abstract = {This paper introduces Streaming Half-Space-Trees (HS-Trees), a fast one-class anomaly detector for evolving data streams. It requires only normal data for training and works well when anomalous data are rare. The model features an ensemble of random HS-Trees, and the tree structure is constructed without any data. This makes the method highly efficient because it requires no model restructuring when adapting to evolving data streams. Our analysis shows that Streaming HS-Trees has constant amortised time complexity and constant memory requirement. When compared with a state-of-the-art method, our method performs favourably in terms of detection accuracy and runtime performance. Our experimental results also show that the detection performance of Streaming HS-Trees is not sensitive to its parameter settings.},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Two},
pages = {1511–1516},
numpages = {6},
location = {Barcelona, Catalonia, Spain},
series = {IJCAI'11}
}
@Article{designing-streaming-alg-for-outlier-detection,
AUTHOR = {Yu, Kangqing and Shi, Wei and Santoro, Nicola},
TITLE = {Designing a Streaming Algorithm for Outlier Detection in Data Mining—An Incrementa Approach},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1261},
URL = {https://www.mdpi.com/1424-8220/20/5/1261},
ISSN = {1424-8220},
ABSTRACT = {To design an algorithm for detecting outliers over streaming data has become an important task in many common applications, arising in areas such as fraud detections, network analysis, environment monitoring and so forth. Due to the fact that real-time data may arrive in the form of streams rather than batches, properties such as concept drift, temporal context, transiency, and uncertainty need to be considered. In addition, data processing needs to be incremental with limited memory resource, and scalable. These facts create big challenges for existing outlier detection algorithms in terms of their accuracies when they are implemented in an incremental fashion, especially in the streaming environment. To address these problems, we first propose C_KDE_WR, which uses sliding window and kernel function to process the streaming data online, and reports its results demonstrating high throughput on handling real-time streaming data, implemented in a CUDA framework on Graphics Processing Unit (GPU). We also present another algorithm, C_LOF, based on a very popular and effective outlier detection algorithm called Local Outlier Factor (LOF) which unfortunately works only on batched data. Using a novel incremental approach that compensates the drawback of high complexity in LOF, we show how to implement it in a streaming context and to obtain results in a timely manner. Like C_KDE_WR, C_LOF also employs sliding-window and statistical-summary to help making decision based on the data in the current window. It also addresses all those challenges of streaming data as addressed in C_KDE_WR. In addition, we report the comparative evaluation on the accuracy of C_KDE_WR with the state-of-the-art SOD_GPU using Precision, Recall and F-score metrics. Furthermore, a t-test is also performed to demonstrate the significance of the improvement. We further report the testing results of C_LOF on different parameter settings and drew ROC and PR curve with their area under the curve (AUC) and Average Precision (AP) values calculated respectively. Experimental results show that C_LOF can overcome the masquerading problem, which often exists in outlier detection on streaming data. We provide complexity analysis and report experiment results on the accuracy of both C_KDE_WR and C_LOF algorithms in order to evaluate their effectiveness as well as their efficiencies.},
DOI = {10.3390/s20051261}
}
@article{anomaly-pattern-detection,
title = {Anomaly pattern detection for streaming data},
journal = {Expert Systems with Applications},
volume = {149},
pages = {113252},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113252},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300774},
author = {Taegong Kim and Cheong Hee Park},
keywords = {Anomaly pattern detection, Control charts, Hypothesis testing, Outlier detection, Streaming data},
abstract = {Outlier detection aims to find a data sample that is different from most other data samples. While outlier detection is performed at an individual instance level, anomaly pattern detection on a data stream means detecting a time point where a pattern to generate data is unusual and significantly different from normal behavior. Beyond predicting the outlierness of individual data samples in a data stream, it can be very useful to detect the occurrence of anomalous patterns in real time. In this paper, we propose a method for anomaly pattern detection in a data stream based on binary classification for outliers and statistical tests on a data stream of binary labels of normal or an outlier. In the first step, by applying the clustering-based outlier detection method, we transform a data stream into a stream of binary values where 0 stands for the prediction as normal data and 1 for outlier prediction. In the second step, anomaly pattern detection is performed on a stream of binary values by two approaches: testing the equality of parameters in the binomial distributions of a reference window and a detection window, and using control charts for the fraction defective. The proposed method obtained the average true positive detection rate of 94\% in simulated experiments using real and artificial data. The experimental results also show that anomaly pattern occurrence can be detected reliably even when outlier detection performance is relatively low.}
}

@inproceedings{gal2016dropout,
author = {Gal, Yarin and Ghahramani, Zoubin},
title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
year = {2016},
publisher = {JMLR.org},
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1050–1059},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}


@misc{Bispham-email:private,
  author        = "Alexander Beattie",
  howpublished  = "[private e-mail]",
  year          = "2021",
  note          = "Receivers: Matti Bispham"
}
@misc{Highnam-email:private,
  author        = "Alexander Beattie",
  howpublished  = "[private e-mail]",
  month         = "09",
  year          = "2021",
  note          = "Receivers: Kate Highnam"
}
@online{BeattieGithub2022,
  author = {Alex Beattie},
  title = {Master's Thesis},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub Repository},
  url = "https://github.com/alexbeattie42/masters-thesis",
}
@ARTICLE{trainsient-stability-9523750,
  author={Rokrok, Ebrahim and Qoria, Taoufik and Bruyere, Antoine and Francois, Bruno and Guillaud, Xavier},
  journal={IEEE Transactions on Power Systems}, 
  title={Transient Stability Assessment and Enhancement of Grid-Forming Converters Embedding Current Reference Saturation as Current Limiting Strategy},
  year={2022},
  volume={37},
  number={2},
  pages={1519-1531},
  doi={10.1109/TPWRS.2021.3107959}}


@misc{bayesian-weight-uncertainty-neural-networks,
  doi = {10.48550/ARXIV.1505.05424},
  url = {https://arxiv.org/abs/1505.05424},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Weight Uncertainty in Neural Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{SHAP-og-paper,
  doi = {10.48550/ARXIV.1705.07874},
  url = {https://arxiv.org/abs/1705.07874},
  author = {Lundberg, Scott and Lee, Su-In},
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Unified Approach to Interpreting Model Predictions},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
